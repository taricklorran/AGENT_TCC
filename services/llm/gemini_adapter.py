# services/llm/gemini_adapter.py
import google.generativeai as genai
from datetime import datetime
from models.schemas import ExecutionContext, ManagerSchema, ToolResult
from typing import List
from config import settings
import json
import logging
import re

class GeminiAdapter:
    def __init__(self):
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.model = settings.GEMINI_MODEL
        self.logger = logging.getLogger(__name__)
        self.system_instruction = self._load_system_instruction()

    def _load_system_instruction(self) -> str:
        """Carrega a instruÃ§Ã£o do sistema do arquivo"""
        try:
            with open("prompts/system_instruction.md", "r", encoding="utf-8") as file:
                return file.read()
        except FileNotFoundError:
            return "VocÃª Ã© um assistente de IA."
        
    def _create_simplified_manager_list(self, managers: List[ManagerSchema]) -> List[dict]:
        """
        Cria uma lista de dicionÃ¡rios simplificada dos managers e suas ferramentas,
        ideal para ser usada em prompts de LLM, economizando tokens.
        """
        simplified_list = []
        for manager in managers:
            if not manager.isActive:
                continue

            manager_info = {
                "manager_id": manager.manager_id,
                "description": manager.description,
                "tools": []
            }

            for agent in manager.agents:
                if not agent.isActive:
                    continue
                
                for tool in agent.tools:
                    if not tool.isActive:
                        continue
                    
                    params_str = ", ".join([f"{p.name}: {p.type}" for p in tool.parameters_mandatory])
                    tool_info = {
                        "name": tool.tool_name,
                        "description": tool.description,
                        "parameters": params_str if params_str else "Nenhum"
                    }
                    manager_info["tools"].append(tool_info)
            
            if manager_info["tools"]:
                simplified_list.append(manager_info)

        return simplified_list

    def generate(self, prompt: str, system_instruction: str = None) -> str:
        try:
            model = genai.GenerativeModel(
                self.model,
                system_instruction=system_instruction if system_instruction else self.system_instruction
            )
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            self.logger.error(f"Erro na geraÃ§Ã£o Gemini: {str(e)}")
            return ""
        
    def consolidate_final_response(self, context: ExecutionContext, formatting_guidelines: List[str]) -> str:
        """
        Gera a resposta final para o usuÃ¡rio, sintetizando todos os resultados
        e seguindo as diretrizes de formataÃ§Ã£o fornecidas.
        """
        
        guidelines_section = ""
        if formatting_guidelines:
            formatted_guidelines = "\n- ".join(formatting_guidelines)
            guidelines_section = (
                "### ðŸ“œ Regras de FormataÃ§Ã£o ObrigatÃ³rias\n"
                "Para construir a resposta final, vocÃª DEVE seguir estas regras de formataÃ§Ã£o para as informaÃ§Ãµes correspondentes:\n"
                f"- {formatted_guidelines}"
            )

        prompt = f"""
        ## ðŸ¤– Persona
        VocÃª Ã© um Redator Chefe de IA, especialista em comunicaÃ§Ã£o. Sua funÃ§Ã£o Ã© pegar dados brutos e rascunhos de uma equipe de agentes de IA e transformar tudo em uma resposta final, clara, coesa e perfeitamente formatada para um usuÃ¡rio humano.

        ---

        ## ðŸ“ Contexto e Dados Recebidos

        ### Pergunta Original do UsuÃ¡rio:
        {context.user_question}

        ### Resultados Brutos das Ferramentas (Fonte da Verdade):
        ```json
        {json.dumps(context.previous_results, indent=2, ensure_ascii=False)}
        ```

        ### RaciocÃ­nio Interno da Equipe (Para seu Contexto):
        ```
        {"\n".join(context.react_history)}
        ```
        ---
        {guidelines_section}
        ---
        ## ðŸŽ¯ Tarefa Final e Regras de Ouro
        Sua tarefa Ã© sintetizar os **Resultados Brutos das Ferramentas** em uma resposta Ãºnica e amigÃ¡vel para o usuÃ¡rio. Siga estas regras rigorosamente:
        1. **Siga as Regras de FormataÃ§Ã£o:** Se a seÃ§Ã£o "Regras de FormataÃ§Ã£o ObrigatÃ³rias" existir, suas regras sÃ£o a prioridade mÃ¡xima para estilizar as informaÃ§Ãµes correspondentes. Se um resultado nÃ£o tiver uma regra, apresente-o de forma clara e legÃ­vel.
        2. **Baseie-se nos Fatos:** Sua resposta deve sintetizar **todas as informaÃ§Ãµes de contexto disponÃ­veis**. Se os `Resultados Brutos das Ferramentas` contiverem dados, eles sÃ£o a fonte primÃ¡ria da verdade. Se estiverem vazios, use o `RaciocÃ­nio Interno da Equipe` para formular sua resposta, pois ele pode conter a conclusÃ£o direta encontrada pelo orquestrador. NÃ£o invente informaÃ§Ãµes que nÃ£o estejam no contexto fornecido.
        3. **Fale com o UsuÃ¡rio:** A resposta final deve ser direcionada ao usuÃ¡rio, nÃ£o um relatÃ³rio tÃ©cnico.
        4. **Lide com Falhas:** Se os resultados indicarem que uma tarefa falhou, informe isso ao usuÃ¡rio de forma simples e direta.
        
        Agora, gere a resposta final para o usuÃ¡rio.
        """
        return self.generate(prompt).strip()

    def decide_next_manager_action(self, context: ExecutionContext, chat_history:list) -> dict:
        """
        Decide a prÃ³xima aÃ§Ã£o para o orquestrador: chamar um manager ou finalizar.
        """
        try:
            with open("prompts/delegator_prompt.md", "r", encoding="utf-8") as file:
                delegator_prompt_template = file.read()
        except FileNotFoundError:
            self.logger.error("Arquivo de prompt 'delegator_prompt.md' nÃ£o encontrado.")
            # Retorna uma resposta de erro que pode ser tratada pelo orquestrador
            return {"decision": "error", "final_answer": "NÃ£o consegui encontrar minhas instruÃ§Ãµes para decidir o prÃ³ximo passo. Por favor, contate o suporte."}

        # Formata os dados do contexto para o prompt
        simplified_managers = self._create_simplified_manager_list(context.available_managers)
        formatted_managers = json.dumps(simplified_managers, indent=2, ensure_ascii=False)

        formatted_results = json.dumps(context.previous_results, indent=2, ensure_ascii=False)
        formatted_react_history = "\n".join(context.react_history) if context.react_history else "Nenhum histÃ³rico de raciocÃ­nio ainda."

        prompt = delegator_prompt_template.format(
            user_id=context.user_id,
            chat_history=chat_history,
            user_input=context.user_question,
            available_managers=formatted_managers,
            previous_results=formatted_results,
            react_history=formatted_react_history,
            current_date=datetime.now().strftime("%d/%m/%Y %H:%M")
        )

        response_text = self.generate(prompt, system_instruction="VocÃª Ã© um orquestrador de IA que responde em JSON.")
        
        try:
            return self.parse_json_response(response_text)
        except json.JSONDecodeError:
            self.logger.error(f"Falha ao decodificar JSON do delegador: {response_text}")
            return {"decision": "final_answer", "final_answer": "Desculpe, tive um problema ao decidir o que fazer a seguir. Tente novamente."}

    def parse_json_response(self, text_response: str) -> dict:
        """Extrai um objeto JSON de uma string, mesmo que haja texto antes ou depois."""
        try:
            match = re.search(r'\{.*\}', text_response, re.DOTALL)
            if match:
                json_str = match.group(0)
                return json.loads(json_str)
            else:
                self.logger.error(f"Nenhum JSON encontrado na resposta: {text_response}")
                raise json.JSONDecodeError("JSON nÃ£o encontrado", text_response, 0)
        except json.JSONDecodeError as e:
            self.logger.error(f"Erro ao decodificar JSON: {e}\nResposta recebida:\n{text_response}")
            raise

    def react_cycle(self, user_id: str, manager: ManagerSchema, context: ExecutionContext, history: list, original_question: str) -> dict:
        """Executa um ciclo completo ReAct (Thought + Action)"""
        with open("prompts/react_cycle_prompt.md", "r", encoding="utf-8") as file:
            prompt_template = file.read()

        history_str = "\n".join(history) if history else "Nenhum histÃ³rico ainda."
        tools_str = self._format_tools(manager)

        prompt = prompt_template.format(
            user_id=user_id,
            manager_id=manager.manager_id,
            manager_description=manager.description,
            step_objective=context.user_question,
            original_user_question=original_question,
            previous_results=json.dumps(context.previous_results, indent=2, ensure_ascii=False),
            history=history_str,
            available_tools=tools_str,
            current_date=datetime.now().strftime("%d/%m/%Y %H:%M")
        )

        response = self.generate(prompt)
        self.logger.debug(f"Resposta ReAct: {response}")

        return self._parse_react_response(response)

    def _format_tools(self, manager: ManagerSchema) -> str:
        """
        Formata as ferramentas ativas de um manager, agrupando por agente,
        para serem usadas em um prompt de LLM.
        """
        agent_tools_info = []

        # Itera sobre os agentes do manager
        for agent in manager.agents:
            if not agent.isActive:
                continue

            agent_tool_strings = []
            # Itera sobre as ferramentas de cada agente
            for tool in agent.tools:
                if not tool.isActive:
                    continue
                
                # Formata a lista de parÃ¢metros de forma concisa
                params_str = ", ".join([f"{p.name}: {p.type}" for p in tool.parameters_mandatory])

                # Cria a string da ferramenta com a formataÃ§Ã£o ToolName(params): description
                agent_tool_strings.append(f"  - {tool.tool_name}({params_str}): {tool.description}")

            # Adiciona o cabeÃ§alho do agente e suas ferramentas Ã  lista principal,
            # apenas se o agente tiver ferramentas ativas.
            if agent_tool_strings:
                agent_tools_info.append(f"Agente: {agent.agent_id} ({agent.description})")
                agent_tools_info.extend(agent_tool_strings)

        return "\n".join(agent_tools_info)

    def _parse_react_response(self, response: str) -> dict:
        result = {"thought": "", "action": "", "final_answer": ""}

        thought_match = re.search(r'\[THOUGHT\]:(.*?)(?=\[ACTION\]|\[FINAL_ANSWER\]|$)', response, re.DOTALL | re.IGNORECASE)
        if thought_match:
            result["thought"] = thought_match.group(1).strip()

        action_match = re.search(r'\[ACTION\]:(.*?)(?=\[THOUGHT\]|\[FINAL_ANSWER\]|$)', response, re.DOTALL | re.IGNORECASE)
        if action_match:
            result["action"] = action_match.group(1).strip()

        final_match = re.search(r'\[FINAL_ANSWER\]:(.*?)(?=\[THOUGHT\]|\[ACTION\]|$)', response, re.DOTALL | re.IGNORECASE)
        if final_match:
            result["final_answer"] = final_match.group(1).strip()

        return result